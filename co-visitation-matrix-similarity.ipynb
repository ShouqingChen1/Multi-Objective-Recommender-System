{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"VER = 5\n\nimport pandas as pd, numpy as np\nfrom tqdm.notebook import tqdm\nimport os, sys, pickle, glob, gc\nfrom collections import Counter\nimport itertools\nimport cudf, itertools\nprint('We will use RAPIDS version',cudf.__version__)","metadata":{"papermill":{"duration":3.036143,"end_time":"2022-11-10T16:03:24.014816","exception":false,"start_time":"2022-11-10T16:03:20.978673","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-09T02:18:57.613769Z","iopub.execute_input":"2023-02-09T02:18:57.614546Z","iopub.status.idle":"2023-02-09T02:19:00.588586Z","shell.execute_reply.started":"2023-02-09T02:18:57.614447Z","shell.execute_reply":"2023-02-09T02:19:00.587347Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"We will use RAPIDS version 21.10.01\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# CACHE FUNCTIONS\ndef read_file(f):\n    return cudf.DataFrame( data_cache[f] )\ndef read_file_to_cache(f):\n    df = pd.read_parquet(f)\n    df.ts = (df.ts/1000).astype('int32')\n    df['type'] = df['type'].map(type_labels).astype('int8')\n    return df\n\n# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\ndata_cache = {}\ntype_labels = {'clicks':0, 'carts':1, 'orders':2}\nfiles = glob.glob('../input/otto-chunk-data-inparquet-format/*_parquet/*')\nfor f in files: data_cache[f] = read_file_to_cache(f)\n\n# CHUNK PARAMETERS\nREAD_CT = 5\nCHUNK = int( np.ceil( len(files)/6 ))\nprint(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')","metadata":{"execution":{"iopub.status.busy":"2023-02-09T02:22:26.792080Z","iopub.execute_input":"2023-02-09T02:22:26.792842Z","iopub.status.idle":"2023-02-09T02:23:28.259432Z","shell.execute_reply.started":"2023-02-09T02:22:26.792803Z","shell.execute_reply":"2023-02-09T02:23:28.258216Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"We will process 146 files, in groups of 5 and chunks of 25.\nCPU times: user 45.6 s, sys: 8.66 s, total: 54.3 s\nWall time: 1min 1s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1) \"Carts Orders\" Co-visitation Matrix - Weighted Similarity","metadata":{"papermill":{"duration":0.004089,"end_time":"2022-11-10T16:03:24.100502","exception":false,"start_time":"2022-11-10T16:03:24.096413","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\ntype_weight = {0:1, 1:6, 2:3}\n\n# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b,READ_CT):\n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b: df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['d'] = np.arange(len(df))\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            # ASSIGN WEIGHTS\n            df = df.drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = df.type_y.map(type_weight)\n            df['wgd'] = 1/np.log( (df.d_x - df.d_y).abs()+1) \n            df['wgts'] = 1/np.log( (df.ts_x - df.ts_y).abs() /60/60 + 1)\n            df['sim'] = df['wgt']* df['wgd'] * df['wgts']\n            df = df[['aid_x','aid_y','sim']]\n            df.sim = df.sim.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).sim.sum()\n            # COMBINE INNER CHUNKS\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        # COMBINE OUTER CHUNKS\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','sim'],ascending=[True,False])\n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n    # SAVE PART TO DISK (convert to pandas first uses less memory)\n    tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')","metadata":{"papermill":{"duration":566.561189,"end_time":"2022-11-10T16:12:50.666123","exception":false,"start_time":"2022-11-10T16:03:24.104934","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-02-09T02:51:45.280121Z","iopub.execute_input":"2023-02-09T02:51:45.280568Z","iopub.status.idle":"2023-02-09T02:55:22.776627Z","shell.execute_reply.started":"2023-02-09T02:51:45.280533Z","shell.execute_reply":"2023-02-09T02:55:22.775448Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 2\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 3\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 4\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \nCPU times: user 2min 17s, sys: 1min 19s, total: 3min 36s\nWall time: 3min 37s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2) \"Buy2Buy\" Co-visitation Matrix- Weighted Similarity","metadata":{}},{"cell_type":"code","source":"%%time\ntype_weight = {0:1, 1:6, 2:3}\n\n# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b,READ_CT):\n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b: df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['d'] = np.arange(len(df))\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            # ASSIGN WEIGHTS\n            df = df.drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = df.type_y.map(type_weight)\n            df['wgd'] = 1/np.log( (df.d_x - df.d_y).abs()+1) \n            df['wgts'] = 1/np.log( (df.ts_x - df.ts_y).abs() /60/60 + 1)\n            df['sim'] = df['wgt']* df['wgd'] * df['wgts']\n            df = df[['aid_x','aid_y','sim']]\n            df.sim = df.sim.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).sim.sum()\n            # COMBINE INNER CHUNKS\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        # COMBINE OUTER CHUNKS\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','sim'],ascending=[True,False])\n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n    # SAVE PART TO DISK (convert to pandas first uses less memory)\n    tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')","metadata":{"execution":{"iopub.status.busy":"2023-02-09T03:02:14.843682Z","iopub.execute_input":"2023-02-09T03:02:14.844472Z","iopub.status.idle":"2023-02-09T03:03:09.012152Z","shell.execute_reply.started":"2023-02-09T03:02:14.844429Z","shell.execute_reply":"2023-02-09T03:03:09.011010Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 2\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 3\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 4\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \nCPU times: user 35.6 s, sys: 18.6 s, total: 54.1 s\nWall time: 54.2 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3) \"Clicks\" Co-visitation Matrix - Weighted Similarity","metadata":{"papermill":{"duration":0.04526,"end_time":"2022-11-10T16:14:44.58589","exception":false,"start_time":"2022-11-10T16:14:44.54063","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b,READ_CT):\n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b: df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['d'] = np.arange(len(df))\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            # ASSIGN WEIGHTS \n            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n            df['d'] = np.arange(len(df))\n            df['wgd'] = 1/log( (df.d_x - df.d_y).abs()+1),\n            df['wgts'] = 1/log( (df.ts_x - df.ts_y).abs() /60/60 + 1)\n            df['sim'] = df['wgt']* df['wgd'] * df['wgts']\n            df = df[['aid_x','aid_y','sim']]\n            df.sim = df.sim.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).sim.sum()\n            # COMBINE INNER CHUNKS\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        # COMBINE OUTER CHUNKS\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','sim'],ascending=[True,False])\n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<20].drop('n',axis=1)\n    # SAVE PART TO DISK (convert to pandas first uses less memory)\n    tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2022-11-10T16:14:44.629032","status":"running"},"tags":[],"execution":{"iopub.status.busy":"2023-02-09T06:45:16.576583Z","iopub.execute_input":"2023-02-09T06:45:16.577004Z","iopub.status.idle":"2023-02-09T06:45:16.664001Z","shell.execute_reply.started":"2023-02-09T06:45:16.576918Z","shell.execute_reply":"2023-02-09T06:45:16.663046Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\n### DISK PART 1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'CHUNK' is not defined"],"ename":"NameError","evalue":"name 'CHUNK' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}